{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.eager as tfe\n",
    "import numpy as np\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n",
      "图片数字：5\n",
      "训练数据 (60000, 784), (60000,)\n",
      "测试数据 (10000, 784), (10000,)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 训练、测试集\n",
    "(train_x, train_y), (test_x, test_y) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# 数据形状和类型变换\n",
    "train_x = train_x.reshape([-1,28*28]).astype('float32')\n",
    "test_x = test_x.reshape([-1,28*28]).astype('float32')\n",
    "train_y = train_y.astype('int32')\n",
    "test_y = test_y.astype('int32')\n",
    "\n",
    "# 显示训练集\n",
    "%pylab inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "imgplot = plt.imshow(train_x[0].reshape([28,28]),cmap='gray')\n",
    "print('图片数字：%s' %train_y[0])\n",
    "print('训练数据 %s, %s' %(train_x.shape,train_y.shape))\n",
    "print('测试数据 %s, %s' %(test_x.shape,test_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 自搭模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化方法\n",
    "def inits(shape):\n",
    "    return tf.random_uniform(shape,\n",
    "            minval=-np.sqrt(5) * np.sqrt(1.0 / shape[0]),\n",
    "            maxval=np.sqrt(5) * np.sqrt(1.0 / shape[0]))\n",
    "# 定义模型\n",
    "class Model(object):\n",
    "    def __init__(self):\n",
    "        # 参数初始化\n",
    "        self.W1 = tfe.Variable(inits([784,512]))\n",
    "        self.b1 = tfe.Variable(inits([512]))\n",
    "        self.W2 = tfe.Variable(inits([512,256]))\n",
    "        self.b2 = tfe.Variable(inits([256]))\n",
    "        self.W3 = tfe.Variable(inits([256,10]))\n",
    "        self.b3 = tfe.Variable(inits([10]))\n",
    "    def __call__(self, x):\n",
    "        # 正向传递\n",
    "        y = tf.nn.relu(tf.matmul(x, self.W1) + self.b1)\n",
    "        y = tf.nn.relu(tf.matmul(y, self.W2) + self.b2)\n",
    "        y = tf.matmul(y, self.W3) + self.b3\n",
    "        return y\n",
    "# 实例模型\n",
    "model = Model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000 | train loss: 0.934 | train acc: 0.880 | test loss: 1.072 | test acc: 0.873\n",
      "Epoch: 001 | train loss: 0.562 | train acc: 0.906 | test loss: 0.729 | test acc: 0.892\n",
      "Epoch: 002 | train loss: 0.401 | train acc: 0.925 | test loss: 0.596 | test acc: 0.905\n",
      "Epoch: 003 | train loss: 0.324 | train acc: 0.933 | test loss: 0.539 | test acc: 0.909\n",
      "Epoch: 004 | train loss: 0.263 | train acc: 0.942 | test loss: 0.492 | test acc: 0.914\n",
      "Epoch: 005 | train loss: 0.223 | train acc: 0.950 | test loss: 0.466 | test acc: 0.917\n",
      "Epoch: 006 | train loss: 0.192 | train acc: 0.954 | test loss: 0.445 | test acc: 0.920\n",
      "Epoch: 007 | train loss: 0.170 | train acc: 0.960 | test loss: 0.428 | test acc: 0.922\n",
      "Epoch: 008 | train loss: 0.149 | train acc: 0.964 | test loss: 0.412 | test acc: 0.925\n",
      "Epoch: 009 | train loss: 0.136 | train acc: 0.965 | test loss: 0.409 | test acc: 0.926\n",
      "Epoch: 010 | train loss: 0.121 | train acc: 0.970 | test loss: 0.399 | test acc: 0.927\n",
      "Epoch: 011 | train loss: 0.108 | train acc: 0.973 | test loss: 0.388 | test acc: 0.928\n",
      "Epoch: 012 | train loss: 0.100 | train acc: 0.974 | test loss: 0.386 | test acc: 0.928\n",
      "Epoch: 013 | train loss: 0.090 | train acc: 0.977 | test loss: 0.380 | test acc: 0.930\n",
      "Epoch: 014 | train loss: 0.083 | train acc: 0.979 | test loss: 0.374 | test acc: 0.933\n",
      "Epoch: 015 | train loss: 0.077 | train acc: 0.981 | test loss: 0.374 | test acc: 0.932\n",
      "Epoch: 016 | train loss: 0.071 | train acc: 0.982 | test loss: 0.370 | test acc: 0.933\n",
      "Epoch: 017 | train loss: 0.066 | train acc: 0.983 | test loss: 0.369 | test acc: 0.934\n",
      "Epoch: 018 | train loss: 0.061 | train acc: 0.985 | test loss: 0.364 | test acc: 0.935\n",
      "Epoch: 019 | train loss: 0.059 | train acc: 0.985 | test loss: 0.364 | test acc: 0.935\n",
      "Epoch: 020 | train loss: 0.054 | train acc: 0.987 | test loss: 0.363 | test acc: 0.937\n",
      "Epoch: 021 | train loss: 0.051 | train acc: 0.987 | test loss: 0.363 | test acc: 0.937\n",
      "Epoch: 022 | train loss: 0.048 | train acc: 0.988 | test loss: 0.360 | test acc: 0.937\n",
      "Epoch: 023 | train loss: 0.045 | train acc: 0.989 | test loss: 0.357 | test acc: 0.939\n",
      "Epoch: 024 | train loss: 0.043 | train acc: 0.989 | test loss: 0.361 | test acc: 0.937\n",
      "Epoch: 025 | train loss: 0.040 | train acc: 0.990 | test loss: 0.359 | test acc: 0.938\n",
      "Epoch: 026 | train loss: 0.038 | train acc: 0.991 | test loss: 0.358 | test acc: 0.939\n",
      "Epoch: 027 | train loss: 0.036 | train acc: 0.992 | test loss: 0.358 | test acc: 0.941\n",
      "Epoch: 028 | train loss: 0.034 | train acc: 0.992 | test loss: 0.357 | test acc: 0.941\n",
      "Epoch: 029 | train loss: 0.032 | train acc: 0.993 | test loss: 0.356 | test acc: 0.940\n",
      "Epoch: 030 | train loss: 0.031 | train acc: 0.993 | test loss: 0.357 | test acc: 0.941\n",
      "Epoch: 031 | train loss: 0.029 | train acc: 0.993 | test loss: 0.356 | test acc: 0.941\n",
      "Epoch: 032 | train loss: 0.027 | train acc: 0.994 | test loss: 0.356 | test acc: 0.942\n",
      "Epoch: 033 | train loss: 0.027 | train acc: 0.995 | test loss: 0.358 | test acc: 0.941\n",
      "Epoch: 034 | train loss: 0.025 | train acc: 0.995 | test loss: 0.356 | test acc: 0.942\n",
      "Epoch: 035 | train loss: 0.024 | train acc: 0.995 | test loss: 0.355 | test acc: 0.941\n",
      "Epoch: 036 | train loss: 0.023 | train acc: 0.995 | test loss: 0.357 | test acc: 0.941\n",
      "Epoch: 037 | train loss: 0.022 | train acc: 0.996 | test loss: 0.357 | test acc: 0.942\n",
      "Epoch: 038 | train loss: 0.021 | train acc: 0.996 | test loss: 0.355 | test acc: 0.943\n",
      "Epoch: 039 | train loss: 0.020 | train acc: 0.996 | test loss: 0.356 | test acc: 0.942\n",
      "Epoch: 040 | train loss: 0.019 | train acc: 0.997 | test loss: 0.358 | test acc: 0.942\n",
      "Epoch: 041 | train loss: 0.018 | train acc: 0.997 | test loss: 0.356 | test acc: 0.942\n",
      "Epoch: 042 | train loss: 0.018 | train acc: 0.997 | test loss: 0.355 | test acc: 0.943\n",
      "Epoch: 043 | train loss: 0.017 | train acc: 0.997 | test loss: 0.356 | test acc: 0.943\n",
      "Epoch: 044 | train loss: 0.016 | train acc: 0.997 | test loss: 0.356 | test acc: 0.943\n",
      "Epoch: 045 | train loss: 0.016 | train acc: 0.997 | test loss: 0.356 | test acc: 0.942\n",
      "Epoch: 046 | train loss: 0.015 | train acc: 0.997 | test loss: 0.357 | test acc: 0.943\n",
      "Epoch: 047 | train loss: 0.015 | train acc: 0.998 | test loss: 0.357 | test acc: 0.944\n",
      "Epoch: 048 | train loss: 0.014 | train acc: 0.998 | test loss: 0.357 | test acc: 0.943\n",
      "Epoch: 049 | train loss: 0.014 | train acc: 0.998 | test loss: 0.358 | test acc: 0.943\n",
      "Final Test Loss: 0.9432\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "# 误差函数\n",
    "def loss(logits, label):\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels=label, logits=logits)\n",
    "    return loss\n",
    "\n",
    "# 更新方式\n",
    "def train(model, x, y, learning_rate, batch_size, epoch):\n",
    "    # 更新次数\n",
    "    for e in range(epoch):\n",
    "        # 批量更新\n",
    "        r = np.random.permutation(len(x))\n",
    "        x = x[r]\n",
    "        y = y[r]\n",
    "        for b in range(0,len(x),batch_size):\n",
    "            # 计算梯度\n",
    "            with tf.GradientTape() as tape:\n",
    "                loss_value = loss(model(x[b:b+batch_size]), y[b:b+batch_size])\n",
    "                dW1, db1, dW2, db2, dW3, db3 = tape.gradient(loss_value, \n",
    "                                   [model.W1, model.b1, model.W2, model.b2, model.W3, model.b3])\n",
    "            # 训练更新\n",
    "            model.W1.assign_sub(dW1 * learning_rate)\n",
    "            model.b1.assign_sub(db1 * learning_rate)\n",
    "            model.W2.assign_sub(dW2 * learning_rate)\n",
    "            model.b2.assign_sub(db2 * learning_rate)\n",
    "            model.W3.assign_sub(dW3 * learning_rate)\n",
    "            model.b3.assign_sub(db3 * learning_rate)\n",
    "            \n",
    "        # 显示\n",
    "        train_p = model(train_x)\n",
    "        test_p = model(test_x)\n",
    "        print(\"Epoch: %03d | train loss: %.3f | train acc: %.3f | test loss: %.3f | test acc: %.3f\" \n",
    "              %(e, loss(train_p, train_y), accuracy_score(tf.argmax(train_p,1), train_y),\n",
    "                   loss(test_p, test_y), accuracy_score(tf.argmax(test_p,1), test_y)))\n",
    "\n",
    "# 训练\n",
    "train(model, train_x, train_y, learning_rate = 0.001, batch_size = 256, epoch = 50)\n",
    "\n",
    "# 评估\n",
    "test_p = model(test_x)\n",
    "print(\"Final Test Loss: %s\" %accuracy_score(tf.argmax(test_p,1), test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 用 API 建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "          tf.keras.layers.Dense(512, activation=tf.nn.relu, input_shape=(784,)),\n",
    "          tf.keras.layers.Dense(256, activation=tf.nn.relu),\n",
    "          tf.keras.layers.Dense(10)\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 用 API 训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 000 | train loss: 0.902 | train acc: 0.909 | test loss: 1.105 | test acc: 0.899\n",
      "Epoch: 001 | train loss: 0.524 | train acc: 0.928 | test loss: 0.796 | test acc: 0.908\n",
      "Epoch: 002 | train loss: 0.366 | train acc: 0.940 | test loss: 0.667 | test acc: 0.913\n",
      "Epoch: 003 | train loss: 0.255 | train acc: 0.957 | test loss: 0.582 | test acc: 0.924\n",
      "Epoch: 004 | train loss: 0.195 | train acc: 0.964 | test loss: 0.538 | test acc: 0.928\n",
      "Epoch: 005 | train loss: 0.152 | train acc: 0.971 | test loss: 0.519 | test acc: 0.930\n",
      "Epoch: 006 | train loss: 0.129 | train acc: 0.974 | test loss: 0.507 | test acc: 0.930\n",
      "Epoch: 007 | train loss: 0.105 | train acc: 0.978 | test loss: 0.491 | test acc: 0.933\n",
      "Epoch: 008 | train loss: 0.086 | train acc: 0.983 | test loss: 0.477 | test acc: 0.933\n",
      "Epoch: 009 | train loss: 0.073 | train acc: 0.985 | test loss: 0.469 | test acc: 0.935\n",
      "Epoch: 010 | train loss: 0.060 | train acc: 0.988 | test loss: 0.466 | test acc: 0.935\n",
      "Epoch: 011 | train loss: 0.052 | train acc: 0.989 | test loss: 0.454 | test acc: 0.937\n",
      "Epoch: 012 | train loss: 0.044 | train acc: 0.991 | test loss: 0.448 | test acc: 0.938\n",
      "Epoch: 013 | train loss: 0.039 | train acc: 0.992 | test loss: 0.448 | test acc: 0.940\n",
      "Epoch: 014 | train loss: 0.033 | train acc: 0.994 | test loss: 0.442 | test acc: 0.939\n",
      "Epoch: 015 | train loss: 0.030 | train acc: 0.994 | test loss: 0.441 | test acc: 0.939\n",
      "Epoch: 016 | train loss: 0.026 | train acc: 0.996 | test loss: 0.439 | test acc: 0.941\n",
      "Epoch: 017 | train loss: 0.025 | train acc: 0.995 | test loss: 0.437 | test acc: 0.939\n",
      "Epoch: 018 | train loss: 0.020 | train acc: 0.997 | test loss: 0.433 | test acc: 0.941\n",
      "Epoch: 019 | train loss: 0.018 | train acc: 0.997 | test loss: 0.434 | test acc: 0.942\n",
      "Epoch: 020 | train loss: 0.016 | train acc: 0.998 | test loss: 0.434 | test acc: 0.941\n",
      "Epoch: 021 | train loss: 0.015 | train acc: 0.998 | test loss: 0.431 | test acc: 0.942\n",
      "Epoch: 022 | train loss: 0.013 | train acc: 0.998 | test loss: 0.429 | test acc: 0.942\n",
      "Epoch: 023 | train loss: 0.012 | train acc: 0.998 | test loss: 0.430 | test acc: 0.943\n",
      "Epoch: 024 | train loss: 0.011 | train acc: 0.999 | test loss: 0.428 | test acc: 0.942\n",
      "Epoch: 025 | train loss: 0.010 | train acc: 0.999 | test loss: 0.429 | test acc: 0.943\n",
      "Epoch: 026 | train loss: 0.009 | train acc: 0.999 | test loss: 0.428 | test acc: 0.943\n",
      "Epoch: 027 | train loss: 0.008 | train acc: 0.999 | test loss: 0.426 | test acc: 0.943\n",
      "Epoch: 028 | train loss: 0.008 | train acc: 0.999 | test loss: 0.426 | test acc: 0.944\n",
      "Epoch: 029 | train loss: 0.008 | train acc: 0.999 | test loss: 0.427 | test acc: 0.943\n",
      "Epoch: 030 | train loss: 0.007 | train acc: 0.999 | test loss: 0.425 | test acc: 0.944\n",
      "Epoch: 031 | train loss: 0.006 | train acc: 0.999 | test loss: 0.426 | test acc: 0.944\n",
      "Epoch: 032 | train loss: 0.007 | train acc: 0.999 | test loss: 0.427 | test acc: 0.944\n",
      "Epoch: 033 | train loss: 0.006 | train acc: 1.000 | test loss: 0.426 | test acc: 0.944\n",
      "Epoch: 034 | train loss: 0.005 | train acc: 1.000 | test loss: 0.426 | test acc: 0.945\n",
      "Epoch: 035 | train loss: 0.005 | train acc: 1.000 | test loss: 0.426 | test acc: 0.945\n",
      "Epoch: 036 | train loss: 0.005 | train acc: 1.000 | test loss: 0.425 | test acc: 0.945\n",
      "Epoch: 037 | train loss: 0.004 | train acc: 1.000 | test loss: 0.425 | test acc: 0.946\n",
      "Epoch: 038 | train loss: 0.004 | train acc: 1.000 | test loss: 0.424 | test acc: 0.945\n",
      "Epoch: 039 | train loss: 0.004 | train acc: 1.000 | test loss: 0.425 | test acc: 0.946\n",
      "Epoch: 040 | train loss: 0.004 | train acc: 1.000 | test loss: 0.425 | test acc: 0.946\n",
      "Epoch: 041 | train loss: 0.004 | train acc: 1.000 | test loss: 0.425 | test acc: 0.945\n",
      "Epoch: 042 | train loss: 0.003 | train acc: 1.000 | test loss: 0.424 | test acc: 0.946\n",
      "Epoch: 043 | train loss: 0.003 | train acc: 1.000 | test loss: 0.425 | test acc: 0.946\n",
      "Epoch: 044 | train loss: 0.003 | train acc: 1.000 | test loss: 0.424 | test acc: 0.946\n",
      "Epoch: 045 | train loss: 0.003 | train acc: 1.000 | test loss: 0.424 | test acc: 0.947\n",
      "Epoch: 046 | train loss: 0.003 | train acc: 1.000 | test loss: 0.425 | test acc: 0.946\n",
      "Epoch: 047 | train loss: 0.003 | train acc: 1.000 | test loss: 0.424 | test acc: 0.947\n",
      "Epoch: 048 | train loss: 0.003 | train acc: 1.000 | test loss: 0.423 | test acc: 0.947\n",
      "Epoch: 049 | train loss: 0.003 | train acc: 1.000 | test loss: 0.423 | test acc: 0.947\n",
      "Final Test Loss: 0.947\n"
     ]
    }
   ],
   "source": [
    "# 误差函数\n",
    "def loss(logits, label):\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels=label, logits=logits)\n",
    "    return loss\n",
    "\n",
    "# 更新方式\n",
    "def train(model, x, y, learning_rate, batch_size, epoch):\n",
    "    # 更新次数\n",
    "    for e in range(epoch):\n",
    "        # 批量更新\n",
    "        r = np.random.permutation(len(x))\n",
    "        x = x[r]\n",
    "        y = y[r]\n",
    "        for b in range(0,len(x),batch_size):\n",
    "            # 计算梯度\n",
    "            with tf.GradientTape() as tape:\n",
    "                loss_value = loss(model(x[b:b+batch_size]), y[b:b+batch_size])\n",
    "                grads = tape.gradient(loss_value, model.variables)\n",
    "            # 训练更新\n",
    "            optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "            optimizer.apply_gradients(zip(grads, model.variables),\n",
    "                            global_step=tf.train.get_or_create_global_step())\n",
    "        # 显示\n",
    "        train_p = model(train_x)\n",
    "        test_p = model(test_x)\n",
    "        print(\"Epoch: %03d | train loss: %.3f | train acc: %.3f | test loss: %.3f | test acc: %.3f\" \n",
    "              %(e, loss(train_p, train_y), accuracy_score(tf.argmax(train_p,1), train_y),\n",
    "                   loss(test_p, test_y), accuracy_score(tf.argmax(test_p,1), test_y)))\n",
    "\n",
    "# 训练\n",
    "train(model, train_x, train_y, learning_rate = 0.001, batch_size = 256, epoch = 50)\n",
    "\n",
    "# 评估\n",
    "test_p = model(test_x)\n",
    "print(\"Final Test Loss: %s\" %accuracy_score(tf.argmax(test_p,1), test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "真实数字：2\n",
      "预测数字：2\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "imgplot = plt.imshow(test_x[1].reshape((28,28)))\n",
    "print('真实数字：%s' %test_y[1])\n",
    "print('预测数字：%s' %tf.argmax(test_p[1]).numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
